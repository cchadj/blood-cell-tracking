{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import collections\n",
    "import os\n",
    "import uuid\n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from patch_extraction import NegativeExtractionMode\n",
    "from patch_extraction import SessionPatchExtractor as PE\n",
    "from patch_extraction import get_parallel_points, get_perpendicular_points\n",
    "from video_session import get_video_sessions\n",
    "\n",
    "from cnnlearning import CNN\n",
    "from cnnlearning import TrainingTracker, train\n",
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_patches\n",
    "from plotutils import no_ticks\n",
    "from plotutils import plot_images_as_grid\n",
    "from IPython.display import  display\n",
    "\n",
    "training_video_sessions = get_video_sessions(marked=True, registered=True, validation=False)\n",
    "print()\n",
    "print('Using cuda:', torch.cuda.is_available())\n",
    "print()\n",
    "# video_sessions = [vs for vs in video_sessions if 'shared-videos' in vs.video_file]\n",
    "print('Training Videos')\n",
    "display([vs.video_file for vs in training_video_sessions])\n",
    "\n",
    "print('Validation Videos')\n",
    "validation_video_sessions = get_video_sessions(marked=True, registered=True, validation=True)\n",
    "# video_sessions = [vs for vs in video_sessions if 'shared-videos' in vs.video_file]\n",
    "[vs.video_file for vs in validation_video_sessions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_uid = 6\n",
    "\n",
    "# hyperparameters\n",
    "\n",
    "patch_size = 23\n",
    "translation_pixels = 0\n",
    "negative_extraction_mode = NegativeExtractionMode.PERPENDICULAR\n",
    "negative_search_radius = 7\n",
    "npp=4\n",
    "\n",
    "to_grayscale = False\n",
    "temporal_width = 0\n",
    "mixed_channel=False\n",
    "model_type=0\n",
    "do_preprocessing=False\n",
    "limit_extraction_to_vessel_mask=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = training_video_sessions[0]\n",
    "frame_idx = list(vs.cell_positions)[0]\n",
    "points = vs.cell_positions[frame_idx]\n",
    "\n",
    "if negative_extraction_mode == NegativeExtractionMode.PARALLEL:\n",
    "    x, y = get_parallel_points(points, negative_search_radius, npp)\n",
    "elif negative_extraction_mode == NegativeExtractionMode.PERPENDICULAR:\n",
    "    x, y = get_perpendicular_points(points, negative_search_radius, npp)\n",
    "    \n",
    "plt.figure(figsize=(10, 10))\n",
    "no_ticks()\n",
    "plt.scatter(points[:, 0], points[:, 1])\n",
    "plt.scatter(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc_mc = 'sc'\n",
    "if mixed_channel:\n",
    "    sc_mc = f'mc{cell_images.shape[-1]}'\n",
    "if temporal_width > 0:\n",
    "    sc_mc = 'tc'\n",
    "\n",
    "if negative_extraction_mode == NegativeExtractionMode.CIRCLE:\n",
    "    negative_extraction_str = 'circ'\n",
    "elif negative_extraction_mode == NegativeExtractionMode.RECTANGLE:\n",
    "    negative_extraction_str = 'rect'\n",
    "elif negative_extraction_mode == NegativeExtractionMode.PERPENDICULAR:\n",
    "    negative_extraction_str = 'perp'\n",
    "elif negative_extraction_mode == NegativeExtractionMode.PARALLEL:\n",
    "    negative_extraction_str = 'par'\n",
    "\n",
    "use_vessel_mask_str = ''\n",
    "if limit_extraction_to_vessel_mask:\n",
    "    use_vessel_mask_str = '-uv-'\n",
    "\n",
    "do_preprocessing_str = ''\n",
    "if do_preprocessing:\n",
    "    do_preprocessing_str = '-pr-'\n",
    "\n",
    "\n",
    "\n",
    "output_meta = {\n",
    "    \"uuid\": str(uuid.uuid4()),\n",
    "    \"notebook_uid\": notebook_uid,\n",
    "    \"hyperparameters\": {\n",
    "        \"patch_size\": patch_size,\n",
    "        \"translation_pixels\": translation_pixels,\n",
    "        \"negative_extraction_str\": negative_extraction_str,\n",
    "        \"negatives_per_positive\": npp,\n",
    "        \"negative_extraction_mode\": negative_extraction_mode,\n",
    "        \"negative_search_radius\": negative_search_radius,\n",
    "        \"temporal_width\": temporal_width,\n",
    "        \"to_grayscale\": to_grayscale,\n",
    "        \"model_type\": model_type,\n",
    "        \"do_preprocessing\": do_preprocessing,\n",
    "        \"limit_extraction_to_vessel_mask\": limit_extraction_to_vessel_mask,    \n",
    "    }\n",
    "}\n",
    "\n",
    "output_path = os.path.join(\n",
    "    'tmp-res', \n",
    "     f'_uid{notebook_uid}-{sc_mc}-npp{npp}-tp{translation_pixels}'\n",
    "     f'-ps{patch_size}-mt{model_type}-rad{negative_search_radius}'\n",
    "     f'{do_preprocessing_str}{use_vessel_mask_str}-{negative_extraction_str}'\n",
    ")\n",
    "print('The results will be saved in: \\n', output_path)\n",
    "display(output_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure registration on training videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vs in training_video_sessions:\n",
    "    vs.load_vessel_masks(True)\n",
    "    vs.visualize_registration(figsize=(15, 10), fontsize=10, linewidth=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure registration on validation videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vs in validation_video_sessions:\n",
    "    vs.load_vessel_masks(True)\n",
    "    vs.visualize_registration(figsize=(15, 10), fontsize=10, linewidth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "for vs in training_video_sessions:\n",
    "    print(vs.frames_oa790.shape)\n",
    "    print(vs.frames_oa850.shape)\n",
    "    print(vs.frames_confocal.shape)\n",
    "    \n",
    "    _, axes = plt.subplots(1, 4, figsize=(25, 7))\n",
    "    no_ticks(axes)\n",
    "    axes[0].imshow(vs.frames_oa790[0])\n",
    "    axes[0].set_title('oa790')\n",
    "    axes[1].imshow(vs.frames_oa850[0])\n",
    "    axes[1].set_title('oa850')\n",
    "    axes[2].imshow(vs.frames_confocal[0])\n",
    "    axes[2].set_title('confocal')\n",
    "    \n",
    "    first_marked_frame_idx = list(vs.cell_positions)[0]\n",
    "    axes[3].imshow(vs.marked_frames_oa790[0])\n",
    "    cell_positions = vs.cell_positions[first_marked_frame_idx]\n",
    "    axes[3].scatter(cell_positions[..., 0], cell_positions[..., 1])\n",
    "    axes[3].set_title('marked oa790')\n",
    "    plt.show()\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['axes.titlesize'] = 10\n",
    "for vs in validation_video_sessions:\n",
    "    print(vs.frames_oa790.shape)\n",
    "    print(vs.frames_oa850.shape)\n",
    "    print(vs.frames_confocal.shape)\n",
    "    \n",
    "    _, axes = plt.subplots(1, 4, figsize=(40, 10))\n",
    "    no_ticks(axes)\n",
    "    axes[0].imshow(vs.frames_oa790[0])\n",
    "    axes[0].set_title('oa790')\n",
    "    axes[1].imshow(vs.frames_oa850[0])\n",
    "    axes[1].set_title('oa850')\n",
    "    axes[2].imshow(vs.frames_confocal[0])\n",
    "    axes[2].set_title('confocal')\n",
    "    \n",
    "    first_marked_frame_idx = list(vs.cell_positions)[0]\n",
    "    axes[3].imshow(vs.marked_frames_oa790[0])\n",
    "    cell_positions = vs.cell_positions[first_marked_frame_idx]\n",
    "    axes[3].scatter(cell_positions[..., 0], cell_positions[..., 1])\n",
    "    axes[3].set_title('marked oa790')\n",
    "    plt.show()\n",
    "    print('-----------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=training_video_sessions,\n",
    "    \n",
    "    limit_to_vessel_mask=limit_extraction_to_vessel_mask,\n",
    "    mixed_channel_patches=mixed_channel,\n",
    "    \n",
    "    temporal_width=temporal_width,\n",
    "    \n",
    "    extraction_mode=PE.ALL_MODE,\n",
    "    negative_extraction_mode=negative_extraction_mode,\n",
    "    negative_patch_search_radius=negative_search_radius,\n",
    "    \n",
    "    n_negatives_per_positive=npp,\n",
    "    patch_size=patch_size + translation_pixels,\n",
    "    \n",
    "    v=False,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "valid_cell_images, valid_non_cell_images, valid_cell_images_marked, valid_non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=validation_video_sessions,\n",
    "    \n",
    "    limit_to_vessel_mask=limit_extraction_to_vessel_mask,\n",
    "    mixed_channel_patches=mixed_channel,\n",
    "    \n",
    "    temporal_width=temporal_width,\n",
    "    \n",
    "    extraction_mode=PE.VALIDATION_MODE,\n",
    "    negative_extraction_mode=negative_extraction_mode,\n",
    "    negative_patch_search_radius=negative_search_radius,\n",
    "    \n",
    "    n_negatives_per_positive=npp,\n",
    "    patch_size=patch_size + translation_pixels,\n",
    "    \n",
    "    v=False,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10])\n",
    "plot_images_as_grid(cell_images_marked[:10])\n",
    "\n",
    "plot_images_as_grid(non_cell_images[:10])\n",
    "plot_images_as_grid(non_cell_images_marked[:10])\n",
    "\n",
    "plot_images_as_grid(valid_cell_images[:10])\n",
    "plot_images_as_grid(valid_cell_images_marked[:10])\n",
    "\n",
    "plot_images_as_grid(valid_non_cell_images[:10])\n",
    "plot_images_as_grid(valid_non_cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Negatives per positive train', len(non_cell_images) / len(cell_images))\n",
    "print('Negatives per positive valid', len(valid_non_cell_images) / len(valid_cell_images))\n",
    "print('Shape train', cell_images.shape, non_cell_images.shape)\n",
    "print('Shape valid', valid_cell_images.shape, valid_non_cell_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, validset = create_dataset_from_patches(\n",
    "    cell_images, \n",
    "    non_cell_images,\n",
    "    \n",
    "    valid_cell_patches=valid_cell_images,\n",
    "    valid_non_cell_patches=valid_non_cell_images,\n",
    "    \n",
    "    random_translation_pixels=translation_pixels,\n",
    "    random_rotation_degrees=0,\n",
    "    center_crop_patch_size=patch_size,\n",
    "    \n",
    "    validset_ratio=0.0000001,\n",
    "    \n",
    "    to_grayscale=to_grayscale,\n",
    "    standardize=True,\n",
    "    standardize_mean=mean,\n",
    "    standardize_std=std,\n",
    "    v=True,\n",
    ")\n",
    "\n",
    "trainset_marked, validset_marked = create_dataset_from_patches(\n",
    "    cell_images_marked, \n",
    "    non_cell_images_marked,\n",
    "    \n",
    "    valid_cell_patches=valid_cell_images_marked,\n",
    "    valid_non_cell_patches=valid_non_cell_images_marked,\n",
    "    \n",
    "    random_translation_pixels=translation_pixels,\n",
    "    random_rotation_degrees=0,\n",
    "    center_crop_patch_size=patch_size,\n",
    "    \n",
    "    validset_ratio=0.0000001,\n",
    "    \n",
    "    to_grayscale=to_grayscale,\n",
    "    standardize=True,\n",
    "    v=True\n",
    ")\n",
    "\n",
    "loader = torch.utils.data.DataLoader(trainset, batch_size=1)\n",
    "loader_marked = torch.utils.data.DataLoader(trainset_marked, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mins = []\n",
    "maxs = []\n",
    "values = []\n",
    "for i, (img, lbl) in enumerate(loader):\n",
    "    mins.append(img.min().item())\n",
    "    maxs.append(img.max().item())\n",
    "    values.append(img.flatten().numpy())\n",
    "np.min(mins), np.max(maxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patch_extraction\n",
    "cell_images.min(), cell_images.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(values), np.std(values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1, 2, 1)\n",
    "for i, (img, lbl) in enumerate(loader_marked):\n",
    "    print('Dataset min, max', img.min().item(), img.max().item())\n",
    "    print('Dataset shape', img.shape)\n",
    "    idx = 0\n",
    "\n",
    "    plt.imshow(img[idx].permute(1, 2, 0)[..., 0].squeeze(), cmap='gray')\n",
    "    plt.suptitle(f'Frame index {idx}')\n",
    "    print('Image min max', img[idx].min().item(), img[idx].max().item())\n",
    "    print('label', lbl[idx].item()) \n",
    "    break\n",
    "    \n",
    "plt.subplot(1, 2, 2)\n",
    "for i, (img, lbl) in enumerate(loader):\n",
    "    print('Dataset min, max', img.min().item(), img.max().item())\n",
    "    print('Dataset shape', img.shape)\n",
    "    idx = 0\n",
    "\n",
    "    plt.imshow(img[idx].permute(1, 2, 0)[..., 0].squeeze(), cmap='gray')\n",
    "    plt.suptitle(f'Frame index {idx}')\n",
    "    print('Image min max', img[idx].min().item(), img[idx].max().item())\n",
    "    print('label', lbl[idx].item()) \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_cuda import test_cuda\n",
    "test_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNN(dataset_sample=trainset, model_type=model_type, output_classes=2).to('cuda')\n",
    "model.train()\n",
    "train_params = collections.OrderedDict(\n",
    "    lr=.001,\n",
    "    weight_decay=.001,\n",
    "    \n",
    "    batch_size=512,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    \n",
    "    early_stop_patience=30,\n",
    "    learning_rate_scheduler_patience=10,\n",
    "    \n",
    "    epochs=250,\n",
    "    shuffle=True,\n",
    "    evaluation_epochs=5,\n",
    "    \n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    ")\n",
    "\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Negatives per positive train', len(non_cell_images) / len(cell_images))\n",
    "print('Negatives per positive valid', len(valid_non_cell_images) / len(valid_cell_images))\n",
    "print('Shape train', cell_images.shape, non_cell_images.shape)\n",
    "print('Shape valid', valid_cell_images.shape, valid_non_cell_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.save(output_path, v=True)\n",
    "print(cell_images.shape, non_cell_images.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output file\", output_path)\n",
    "\n",
    "print('Loading results...')\n",
    "results = TrainingTracker.from_file(os.path.join(output_path, 'results.pkl'))\n",
    "print('Done')\n",
    "print('Best balanced validation performance')\n",
    "display(results.recorded_models['best_valid_balanced_accuracy']['valid_classification_results'])\n",
    "\n",
    "print('Best balanced training performance')\n",
    "display(results.recorded_models['best_train_balanced_accuracy']['train_classification_results'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
