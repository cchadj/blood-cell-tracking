{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_patches\n",
    "from imageprosessing import hist_match_images, enhance_motion_contrast, normalize_data, gaussian_blur_stack\n",
    "from imageprosessing import enhance_motion_contrast_de_castro, enhance_motion_contrast_j_tam, SessionPreprocessor\n",
    "from sharedvariables import get_video_sessions\n",
    "from video_session import VideoSession\n",
    "from cnnlearning import CNN\n",
    "from patchextraction import SessionPatchExtractor as PE\n",
    "from learningutils import ImageDataset\n",
    "from classificationutils import create_probability_map\n",
    "\n",
    "from cnnlearning import TrainingTracker, train\n",
    "import os\n",
    "import collections\n",
    "\n",
    "import scipy\n",
    "import skimage\n",
    "from skimage.morphology import binary_dilation as bd\n",
    "from skimage.exposure import equalize_adapthist\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "import copy\n",
    "\n",
    "from patchextraction import extract_patches, SessionPatchExtractor\n",
    "from patchextraction import SessionPatchExtractor as PE\n",
    "from imageprosessing import ImageRegistrator\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True)\n",
    "# video_sessions = [vs for vs in video_sessions if 'shared-videos' in vs.video_file]\n",
    "[vs.video_file for vs in video_sessions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing files\n",
    "\n",
    "Every file under any folder in ./data is parsed and put into dictionaries that group videos of the same source.\n",
    "Videos of the same source are considered videos coming from the same Subject, same Session, same OD/OS, same (x, y),\n",
    "same type (Confocal, OA790, OA850).\n",
    "\n",
    "The parsing is case insensitive with the following rules:\n",
    "\n",
    "**unmarked videos**:\n",
    "must not contain 'mask' or '_marked'\n",
    "\n",
    "**marked videos**:\n",
    "Must end with '_marked.\\<<file_extension\\>>'\n",
    "\n",
    "**standard deviation images**:\n",
    "Must end with\n",
    "'_std.\\<<file_extension\\>>'\n",
    "\n",
    "**vessel mask images**: \n",
    "Must end with\n",
    "'_vessel_mask.\\<<file_extension\\>>'\n",
    "\n",
    "**channel type**:\n",
    "must contain one of 'OA790', 'OA850', 'Confocal' (case insensitive)\n",
    "\n",
    "### Example to using VideoSession objects and the get_video_session() function\n",
    "(useful for training and testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all marked and registered videos for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from os.path import basename\n",
    "\n",
    "train_uids = []\n",
    "video_sessions = get_video_sessions(marked=True, registered=True)\n",
    "for session in video_sessions:\n",
    "        assert session.has_marked_video\n",
    "        assert session.is_registered\n",
    "        assert session.has_marked_cells\n",
    "        assert session.uid not in train_uids\n",
    "        train_uids.append(session.uid)\n",
    "        print('-----------------------')\n",
    "        print('Video file:', basename(session.video_file))\n",
    "        print('Uid', session.uid)\n",
    "        print('Does video have a corresponding marked video?:', session.has_marked_video)\n",
    "        print('Subject number:', session.subject_number)\n",
    "        print('Session number:', session.session_number)\n",
    "        print('Marked Video OA790:', basename(session.marked_video_oa790_file))\n",
    "        print('Std dev image confocal:', basename(session.std_image_confocal_file))\n",
    "        print('Std dev image OA850:', basename(session.std_image_oa850_file))\n",
    "        print('Vessel mask OA850:', basename(session.vessel_mask_oa850_file))\n",
    "        print('Vessel mask confocal:', basename(session.vessel_mask_confocal_file))\n",
    "        print('Cell position csv files:', *[basename(f) for f in session.cell_position_csv_files], sep='\\n')\n",
    "        print()\n",
    "        \n",
    "print('Number of video sessions ', len(video_sessions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from os.path import basename\n",
    "\n",
    "train_uids = [vs.uid for vs in get_video_sessions(marked=True, validation=False)]\n",
    "valid_uids =[]\n",
    "video_sessions = get_video_sessions(marked=True, registered=True, validation=True)\n",
    "for session in video_sessions:\n",
    "        assert session.has_marked_video\n",
    "        assert session.is_registered\n",
    "        assert session.has_marked_cells\n",
    "        assert session.uid not in valid_uids, f'Warining, Duplicate video session {session.uid}'\n",
    "        assert session.uid not in train_uids, 'Warning, validation video exist in training videos as well'\n",
    "        \n",
    "        valid_uids.append(session.uid)\n",
    "        print('-----------------------')\n",
    "        print('Video file:', basename(session.video_file))\n",
    "        print('Uid', session.uid)\n",
    "        print('Does video have a corresponding marked video?:', session.has_marked_video)\n",
    "        print('Subject number:', session.subject_number)\n",
    "        print('Session number:', session.session_number)\n",
    "        print('Marked Video OA790:', basename(session.marked_video_oa790_file))\n",
    "        print('Std dev image confocal:', basename(session.std_image_confocal_file))\n",
    "        print('Std dev image OA850:', basename(session.std_image_oa850_file))\n",
    "        print('Vessel mask OA850:', basename(session.vessel_mask_oa850_file))\n",
    "        print('Vessel mask confocal:', basename(session.vessel_mask_confocal_file))\n",
    "        print('Cell position csv files:', *[basename(f) for f in session.cell_position_csv_files], sep='\\n')\n",
    "        print()\n",
    "        \n",
    "print('Number of video sessions ', len(video_sessions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading frames from videos - (and cell positions for each frame)\n",
    "\n",
    "You can get access to the frames of the video session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import no_ticks\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True)\n",
    "_, axes = plt.subplots(1, 2, figsize=(20, 7))\n",
    "no_ticks(axes)\n",
    "\n",
    "axes[0].imshow(session.frames_oa790[0], cmap='gray')\n",
    "axes[0].set_title(f\"First frame of {basename(session.video_oa790_file)}\", fontsize=10)\n",
    "axes[0].scatter(session.cell_positions[0][:, 0], session.cell_positions[0][:, 1], label='cell positions', s=10)\n",
    "axes[0].legend()\n",
    "    \n",
    "axes[1].imshow(session.marked_frames_oa790[0], cmap='gray')\n",
    "axes[1].set_title(f\"First marked frame of {basename(session.marked_video_oa790_file)}\", fontsize=10)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to extract cell and no cell patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SessionPatchExtractor (Object oriented way) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple patch extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Circle search negative patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from generate_datasets import create_cell_and_no_cell_patches\n",
    "from patchextraction import SessionPatchExtractor as PE\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True, validation=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(\n",
    "    vs, \n",
    "    patch_size=21, \n",
    "    n_negatives_per_positive=32,\n",
    "    use_vessel_mask=False, \n",
    "    extraction_mode=PE.ALL_MODE)\n",
    "\n",
    "print(patch_extractor.cell_patches_oa790.shape)\n",
    "print(patch_extractor.non_cell_patches_oa790.shape)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.non_cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_non_cell_patches_oa790[:10])\n",
    "patch_extractor.visualize_patch_extraction(linewidth=2, s=100, frame_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Rectancle search negative patch extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from generate_datasets import create_cell_and_no_cell_patches\n",
    "from patchextraction import SessionPatchExtractor as PE\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True, validation=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(\n",
    "    vs, \n",
    "    patch_size=21, \n",
    "    n_negatives_per_positive=32,\n",
    "    use_vessel_mask=False, \n",
    "    negative_extraction_mode=PE.RECTANGLE,\n",
    "    negative_patch_extraction_radius=33,\n",
    "    extraction_mode=PE.ALL_MODE)\n",
    "\n",
    "print(patch_extractor.cell_patches_oa790.shape)\n",
    "print(patch_extractor.non_cell_patches_oa790.shape)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.non_cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_non_cell_patches_oa790[:10])\n",
    "patch_extractor.visualize_patch_extraction(linewidth=2, s=100, frame_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restricting negatives within vessel mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor = SessionPatchExtractor(\n",
    "    vs, \n",
    "    patch_size=21, \n",
    "    n_negatives_per_positive=32,\n",
    "    use_vessel_mask=True)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.non_cell_patches_oa790[:10])\n",
    "plot_images_as_grid(patch_extractor.marked_non_cell_patches_oa790[:10])\n",
    "patch_extractor.visualize_patch_extraction(linewidth=2, s=100, frame_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal patches\n",
    "\n",
    "Temporal patches include the patches from the same positions from the next and previous frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, validation=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(vs, patch_size=21, temporal_width=1, n_negatives_per_positive=7)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.temporal_cell_patches_oa790[:10], title='Temporal cell patches temporal width 1')\n",
    "plot_images_as_grid(patch_extractor.temporal_marked_cell_patches_oa790[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.temporal_non_cell_patches_oa790[:10], title='Temporal non cell patches temporal width 1')\n",
    "plot_images_as_grid(patch_extractor.temporal_marked_non_cell_patches_oa790[:10])\n",
    "\n",
    "# A higher temporal width will give patches with more channells\n",
    "patch_extractor.temporal_width = 1\n",
    "print(f'Temporal patches shape with temporal width = 1: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 4\n",
    "print(f'Temporal patches shape with temporal width = 4: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 5\n",
    "print(f'Temporal patches shape with temporal width = 5: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "patch_extractor.temporal_width = 6\n",
    "print(f'Temporal patches shape with temporal width = 6: {patch_extractor.temporal_cell_patches_oa790.shape}')\n",
    "print(f'As temporal window becomes bigger notice that there are less patches.')\n",
    "patch_extractor.temporal_width = 1\n",
    "patch_extractor.visualize_temporal_patch_extraction()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed channel patches\n",
    " \n",
    "Mixed channel patches give patches with 3 channels, the first channel is confocal video patch, second channel is from the oa780 channel,\n",
    "third channel is from the oa850 channel.\n",
    "\n",
    "The confocal video and the oa790 channel have the capillaries at the same position. The oa850 video has a vertical displacement, the video is registered before extracting the patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registration process\n",
    "\n",
    "The vessel mask for the 790nm and 850nm video is created and then registered vertically by maximising Dice's coefficient\n",
    "which is a similarity measure usually used to evaluate segmenation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True)\n",
    "vs = video_sessions[1]\n",
    "\n",
    "vs.visualize_registration()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor = SessionPatchExtractor(vs, patch_size=21)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_cell_patches[:10])\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_marked_cell_patches[:10])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_non_cell_patches[:10])\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_marked_non_cell_patches[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Showing the patches for each channel\n",
    "\n",
    "The first image is the patches extracted from the confocal video.\n",
    "Second is the patches extracted from the 790nm video.\n",
    "Third is the patches extracted from the 850nm video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplots(1, 3, figsize=(150, 150))\n",
    "for i, ax in enumerate(axes):\n",
    "    patch_extractor.visualize_mixed_channel_patch_extraction(frame_idx=20, channel=i, ax=axes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Patch Extractor mode - Training, Validation, and All patches\n",
    "\n",
    "Each session has a frame that is assigned for validation.\n",
    "Usually this is the frame with the most cell positions but it can be changed with vs.validation_frame_idx = index\n",
    "\n",
    "Patch extractor has mode for extracting only training and validation patches but the default mode extracts all the patches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor as PE\n",
    "from plotutils import plot_images_as_grid, no_ticks\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True)\n",
    "vs = video_sessions[2]\n",
    "\n",
    "patch_extractor = PE(vs, patch_size=21, extraction_mode=PE.VALIDATION_MODE)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "print('Validation frame index', vs.validation_frame_idx)\n",
    "ax.imshow(vs.frames_oa790[vs.validation_frame_idx], cmap='gray')\n",
    "ax.set_title(f'Validation frame {vs.validation_frame_idx}')\n",
    "cell_positions = vs.cell_positions[vs.validation_frame_idx]\n",
    "ax.scatter(cell_positions[:, 0], cell_positions[:, 1])\n",
    "no_ticks()\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790, title=f'Validation patches. {patch_extractor.cell_patches_oa790.shape}')\n",
    "patch_extractor.visualize_patch_extraction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor.extraction_mode = PE.TRAINING_MODE\n",
    "\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790[:10], title=f'Training patches {patch_extractor.cell_patches_oa790.shape}')\n",
    "patch_extractor.visualize_patch_extraction()\n",
    "\n",
    "patch_extractor.extraction_mode = PE.ALL_MODE\n",
    "print('All mode patches:', patch_extractor.cell_patches_oa790.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset convenience functions\n",
    "\n",
    "Extract all patches (marked and unmarkded) for all the video session given.\n",
    "\n",
    "If no video sessions given then all the video sessions automatically created from the videos with cell position csv in the data folder are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=reg_video_sessions,                                                                  \n",
    "    n_negatives_per_positive=1,                                                                                                \n",
    "    v=True,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10])\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    temporal_width=1,\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title='Temporal width 1')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed channel patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    mixed_channel_patches=True,\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title='Mixed channel patches')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Modes - Training, Validation, All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor as SE\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=video_sessions,                                                                  \n",
    "    n_negatives_per_positive=1,\n",
    "    \n",
    "    extraction_method=SE.ALL_MODE, # Default mode\n",
    "    \n",
    "    v=True,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title=f'All mode patches. {cell_images.shape}')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor as SE\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True, validation=True)\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=video_sessions,\n",
    "    use_vessel_mask=True,\n",
    "    patch_size=21,\n",
    "    n_negatives_per_positive=32,\n",
    "    \n",
    "    extraction_mode=SE.VALIDATION_MODE,\n",
    "    \n",
    "    v=True,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title=f'Validation mode patches. {cell_images.shape}')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summage = 0\n",
    "for vs in video_sessions:\n",
    "    summage += len(np.concatenate([vs.cell_positions[idx] for idx in vs.cell_positions], axis=0))\n",
    "summage, len(cell_images), len(non_cell_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5674 / 623"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor as SE\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=video_sessions,                                                                  \n",
    "    n_negatives_per_positive=1,\n",
    "    \n",
    "    extraction_method=SE.TRAINING_MODE,\n",
    "    \n",
    "    v=True,\n",
    "    vv=False\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images[:10], title=f'Training mode patches. {cell_images.shape}')\n",
    "plot_images_as_grid(cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import plot_images_as_grid\n",
    "\n",
    "reg_video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=reg_video_sessions,                                                                                                                                                                \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageprosessing import hist_match_images\n",
    "\n",
    "def get_highest_contrast_frame(video_sessions):\n",
    "    max_diff = 0\n",
    "    max_diff_idx = 0\n",
    "    for i, vs in enumerate(video_sessions):\n",
    "        vs.mask_frames_oa790 = crop_mask(vs.mask_frames_oa790, 15)\n",
    "        diff = vs.masked_frames_oa790[0].max() - vs.masked_frames_oa790[0].min()\n",
    "        if diff > max_diff:\n",
    "            max_diff_idx = i\n",
    "            max_diff = diff\n",
    "            \n",
    "    highest_contrast_frame = video_sessions[max_diff_idx].masked_frames_oa790[0]\n",
    "    highest_contrast_frame = highest_contrast_frame.filled(highest_contrast_frame.mean())\n",
    "    \n",
    "    return highest_contrast_frame\n",
    "\n",
    "template_frame = get_highest_contrast_frame(reg_video_sessions)\n",
    "\n",
    "hist_matched_cell_images = hist_match_images(cell_images, template_frame)\n",
    "hist_matched_non_cell_images = hist_match_images(non_cell_images, template_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_as_grid(hist_matched_cell_images[:10])\n",
    "plot_images_as_grid(hist_matched_non_cell_images[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from cnnlearning import CNN, train, TrainingTracker \n",
    "\n",
    "standardize_dataset = True\n",
    "trainset, validset = create_dataset_from_cell_and_no_cell_images(hist_matched_cell_images, \n",
    "                                                                 hist_matched_non_cell_images,\n",
    "                                                                 validset_ratio=0.2,\n",
    "                                                                 standardize=True)\n",
    "\n",
    "model = CNN(dataset_sample=trainset, output_classes=2).to('cuda')\n",
    "train_params = collections.OrderedDict(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=.001, weight_decay=0.01),\n",
    "    batch_size=256,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    early_stop_patience=40,\n",
    "    learning_rate_scheduler_patience=20,\n",
    "    epochs=200,\n",
    "    shuffle=True,\n",
    "    # valid_untrunsformed_normals = valid_untrunsformed_normals,\n",
    "    evaluation_epochs=5,\n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    ")\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classificationutils import classify_labeled_dataset, classify_images\n",
    "\n",
    "model = results.recorded_model\n",
    "model.eval()\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, model)\n",
    "positive_accuracy = classify_images(cell_images, model, standardize_dataset=standardize_dataset).sum().item() / len(cell_images)\n",
    "negative_accuracy = (1 - classify_images(non_cell_images, model, standardize_dataset=standardize_dataset)).sum().item() / len(non_cell_images)\n",
    "\n",
    "print()\n",
    "print(f'Model trained on {len(cell_images)} cell patches and {len(non_cell_images)} non cell patches.')\n",
    "print()\n",
    "print('Brief evaluation - best validation accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')\n",
    "\n",
    "train_model = results.recorded_train_model\n",
    "train_model.eval()\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, train_model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, train_model)\n",
    "positive_accuracy = classify_images(cell_images, train_model, standardize_dataset=standardize_dataset).sum().item() / len(cell_images)\n",
    "negative_accuracy = (1 - classify_images(non_cell_images, train_model, standardize_dataset=standardize_dataset)).sum().item() / len(non_cell_images)\n",
    "\n",
    "print()\n",
    "print('Brief evaluation - best training accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_train_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motion contrast enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from plotutils import plot_images_as_grid\n",
    "from generate_datasets import create_cell_and_no_cell_patches, create_dataset_from_cell_and_no_cell_images\n",
    "from imageprosessing import hist_match_images\n",
    "from sharedvariables import get_video_sessions\n",
    "from video_session import SessionPreprocessor\n",
    "from imageprosessing import enhance_motion_contrast\n",
    "import numpy as np\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)[:3]\n",
    "\n",
    "for vs in video_sessions:\n",
    "    pr = SessionPreprocessor(vs, [\n",
    "        lambda frames: enhance_motion_contrast(frames, adapt_hist=False, normalize=True, sigma = 0.75),\n",
    "        lambda frames: np.uint8(frames * 255)\n",
    "    ])\n",
    "    pr.apply_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_images, non_cell_images, _, _ = create_cell_and_no_cell_patches(\n",
    "    video_sessions=video_sessions,                                                                           \n",
    "    n_negatives_per_positive=2,                     \n",
    "    patch_size=29,\n",
    "    v=True,\n",
    "    vv=True\n",
    ")\n",
    "\n",
    "plot_images_as_grid(cell_images)\n",
    "plot_images_as_grid(non_cell_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from cnnlearning import CNN, train, TrainingTracker \n",
    "from generate_datasets import create_dataset_from_cell_and_no_cell_images\n",
    "import torchvision\n",
    "import PIL\n",
    "\n",
    "\n",
    "trainset, validset = create_dataset_from_cell_and_no_cell_images(cell_images, \n",
    "                                                                 non_cell_images, \n",
    "                                                                 standardize=True,\n",
    "                                                                 \n",
    "                                                                 apply_data_augmentation_transformations=True,\n",
    "#                                                                  translation_pixels=2,\n",
    "                                                                 patch_size=21)\n",
    "\n",
    "print('Trainset size', len(trainset), 'Validset size', len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loader = torch.utils.data.DataLoader(validset, batch_size=len(validset))\n",
    "for i, (img, lbl) in enumerate(loader):\n",
    "    print('Dataset min, max', img.min().item(), img.max().item())\n",
    "    print('Dataset shape', img.shape)\n",
    "    idx = 0\n",
    "    plt.imshow(img[idx].squeeze(), cmap='gray')\n",
    "    plt.suptitle(f'Frame index {idx}')\n",
    "    print('Image min max', img[idx].min().item(), img[idx].max().item())\n",
    "    print('label', lbl[idx].item()) \n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(dataset_sample=trainset, output_classes=2).to('cuda')\n",
    "model.train()\n",
    "train_params = collections.OrderedDict(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=.001, weight_decay=.001),\n",
    "    \n",
    "    batch_size=512,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    early_stop_patience=30,\n",
    "    learning_rate_scheduler_patience=10,\n",
    "    epochs=250,\n",
    "    shuffle=True,\n",
    "    evaluation_epochs=5,\n",
    "    \n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    ")\n",
    "\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(),\n",
    "#                                  criterion=torch.nn.CrossEntropyLoss(torch.tensor([.2, 1.]).cuda()),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single channel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import plot_images_as_grid\n",
    "from generate_datasets import create_cell_and_no_cell_patches\n",
    "\n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)[:3]\\\n",
    "\n",
    "cell_images, non_cell_images, cell_images_marked, non_cell_images_marked = \\\n",
    "    create_cell_and_no_cell_patches(patch_size=21,\n",
    "                                    temporal_width=0,\n",
    "                                    \n",
    "                                    mixed_channel_patches=False,\n",
    "                                    \n",
    "                                    video_sessions=video_sessions,\n",
    "                                    n_negatives_per_positive=1,\n",
    "                                    v=False, vv=False)\n",
    "\n",
    "plot_images_as_grid(cell_images[:10])\n",
    "plot_images_as_grid(cell_images_marked[:10])\n",
    "plot_images_as_grid(non_cell_images[:10])\n",
    "plot_images_as_grid(non_cell_images_marked[:10])\n",
    "cell_images.shape, non_cell_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from cnnlearning import CNN, train, TrainingTracker \n",
    "from generate_datasets import create_dataset_from_cell_and_no_cell_images\n",
    "import torchvision\n",
    "import PIL\n",
    "\n",
    "# From the mixed channels we pick only the oa780 and oa850 channels\n",
    "trainset, validset = create_dataset_from_cell_and_no_cell_images(cell_images, \n",
    "                                                                 non_cell_images, \n",
    "                                                                 standardize=True,\n",
    "                                                                 \n",
    "#                                                                  apply_data_augmentation_transformations=True,\n",
    "#                                                                  translation_pixels=2,\n",
    "                                                                 patch_size=21,\n",
    "                                                                )\n",
    "# positive_dataset = LabeledImageDataset(cell_images_marked,     np.ones(len(cell_images), dtype=np.int))\n",
    "# negative_dataset = LabeledImageDataset(non_cell_images_marked, np.zeros(len(non_cell_images), dtype=np.int))\n",
    "\n",
    "\n",
    "print('Trainset size', len(trainset), 'Validset size', len(validset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "loader = torch.utils.data.DataLoader(validset, batch_size=len(validset))\n",
    "for i, (img, lbl) in enumerate(loader):\n",
    "    print(img.min().item(), img.max().item())\n",
    "    print(img.shape)\n",
    "    idx = -5\n",
    "    plt.imshow(img[idx].squeeze(), cmap='gray')\n",
    "    print(img[idx].min(), img[idx].max())\n",
    "    print('label', lbl[idx].item())\n",
    "#     print(lbl)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(dataset_sample=trainset, output_classes=2).to('cuda')\n",
    "model.train()\n",
    "train_params = collections.OrderedDict(\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=.001, weight_decay=.001),\n",
    "    \n",
    "    batch_size=512,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    early_stop_patience=30,\n",
    "    learning_rate_scheduler_patience=10,\n",
    "    epochs=250,\n",
    "    shuffle=True,\n",
    "    evaluation_epochs=5,\n",
    "    \n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    ")\n",
    "\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(torch.tensor([.2, 1.]).cuda()),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classificationutils import classify_labeled_dataset, classify_images\n",
    "from learningutils import LabeledImageDataset, ImageDataset\n",
    "from imageprosessing import center_crop_images\n",
    "import numpy as np\n",
    "model = results.recorded_model\n",
    "model.eval()\n",
    "\n",
    "positive_dataset = LabeledImageDataset(center_crop_images(cell_images, 21),     np.ones(len(cell_images), dtype=np.int))\n",
    "negative_dataset = LabeledImageDataset(center_crop_images(non_cell_images, 21), np.zeros(len(non_cell_images), dtype=np.int))\n",
    "\n",
    "_, train_accuracy, positive_accuracy, negative_accuracy = \\\n",
    "classify_labeled_dataset(trainset, model, ret_pos_and_neg_acc=True)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, model)\n",
    "\n",
    "print()\n",
    "print(f'Model trained on {len(cell_images)} cell patches and {len(non_cell_images)} non cell patches.')\n",
    "print()\n",
    "print('Brief evaluation - best validation accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Convenience Function\n",
    "\n",
    "This function can load a trained model from cache with the required parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_model import train_model_demo\n",
    "from sharedvariables import get_video_sessions\n",
    "import collections\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "    \n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)\n",
    "\n",
    "train_params = collections.OrderedDict(\n",
    "    lr=.001,\n",
    "    weight_decay=.001,\n",
    "    \n",
    "    batch_size=512,\n",
    "    do_early_stop=True,  # Optional default True\n",
    "    \n",
    "    early_stop_patience=30,\n",
    "    learning_rate_scheduler_patience=10,\n",
    "    \n",
    "    n_negatives_per_positive=5,\n",
    "    \n",
    "    epochs=250,\n",
    "    shuffle=True,\n",
    "    evaluation_epochs=5,\n",
    ")\n",
    "\n",
    "model, results = train_model_demo(\n",
    "        video_sessions=video_sessions[:2], # The video sessions the data will be created from\n",
    "\n",
    "        patch_size=27,\n",
    "        temporal_width=0,\n",
    "    \n",
    "        mixed_channel_patches=True,\n",
    "        drop_confocal_channel=True,\n",
    "    \n",
    "        do_hist_match=False,\n",
    "        n_negatives_per_positive=5,\n",
    "\n",
    "        standardize_dataset=True,\n",
    "        apply_data_augmentation_to_dataset=False,\n",
    "        valid_ratio=0.2,\n",
    "    \n",
    "        try_load_data_from_cache=False, # If true attemps to load data from cache. If false just created new (and overwrites old if exist)\n",
    "        try_load_model_from_cache=False, # Attemps to load model from cache. If false creates new\n",
    "    \n",
    "        train_params=train_params, # The training train parameters, uses default if not specified\n",
    "        additional_displays=None,\n",
    "        v=True, vv=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "model_tmp = copy.deepcopy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create probability map from mixed channel patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cnnlearning import TrainingTracker\n",
    "import os\n",
    "from sharedvariables import CACHED_MODELS_FOLDER, get_video_sessions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "\n",
    "plt.subplot(131)\n",
    "plt.imshow(vs.vessel_mask_confocal)\n",
    "plt.subplot(132)\n",
    "plt.imshow(vs.vessel_mask_oa850)\n",
    "plt.subplot(133)\n",
    "plt.imshow(vs.registered_vessel_mask_oa850)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import skimage\n",
    "from skimage.morphology import binary_dilation as bd\n",
    "plt.imshow(scipy.ndimage.morphology.binary_dilation(vs.registered_vessel_mask_oa850, iterations=15))\n",
    "\n",
    "final_image = vs.registered_vessel_mask_oa850\n",
    "for i in range(2):\n",
    "    final_image = bd(final_image)\n",
    "plt.imshow(final_image)\n",
    "scipy.ndimage.morphology.binary_dilation(vs.registered_vessel_mask_oa850, iterations=15) == final_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from patchextraction import extract_patches\n",
    "from imageprosessing import ImageRegistator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)\n",
    "vs = video_sessions[0]\n",
    "padding = cv2.BORDER_REPLICATE\n",
    "\n",
    "mask = np.bool8(final_image)\n",
    "mask &= np.bool8(vs.registered_mask_frames_oa850[0])\n",
    "# mask = np.bool8(np.ones_like(vs.frames_oa790[0]))\n",
    "mask_flattened = mask.reshape(-1)\n",
    "vessel_pixel_indices = np.where(mask_flattened)[0]\n",
    "\n",
    "patches_oa790 = extract_patches(vs.frames_oa790[0], patch_size=27, padding=padding)[vessel_pixel_indices]\n",
    "patches_oa850 = extract_patches(vs.registered_frames_oa850[0], patch_size=27, padding=padding)[vessel_pixel_indices]\n",
    "patches = np.empty_like(patches_oa790, shape=(*patches_oa850.shape[:-1], 2))\n",
    "patches[..., 0] = patches_oa790.squeeze()\n",
    "patches[..., 1] = patches_oa850.squeeze()\n",
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from learningutils import ImageDataset\n",
    "@torch.no_grad()\n",
    "def get_label_probability(images, model, standardize=True, to_grayscale=False, n_output_classes=2, device='cuda'):\n",
    "    \"\"\" Make a prediction for the images giving probabilities for each labels.\n",
    "\n",
    "    Arguments:\n",
    "        images -- NxHxWxC or NxHxW. The images\n",
    "        model  -- The model to do the prediction\n",
    "\n",
    "    Returns:\n",
    "        Returns the probability per label for each image.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model = model.eval()\n",
    "\n",
    "    if len(images.shape) == 3:\n",
    "        # Add channel dimension when images are single channel grayscale\n",
    "        # i.e (Nx100x123 -> Nx100x123x1)\n",
    "        images = images[..., None]\n",
    "\n",
    "    image_dataset = ImageDataset(images, standardize=standardize, to_grayscale=to_grayscale)\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        image_dataset,\n",
    "        batch_size=1024 * 3,\n",
    "    )\n",
    "\n",
    "    c = 0\n",
    "    predictions = torch.empty((len(image_dataset), n_output_classes), dtype=torch.float32)\n",
    "    for images in loader:\n",
    "        images = images.to(device)\n",
    "\n",
    "        print('_-_-_-_-_-_-_-_-_')\n",
    "        print(images.shape)\n",
    "        pred = model(images)\n",
    "        pred = torch.nn.functional.softmax(pred, dim=1)\n",
    "#         print('_-_-_-_-_-_-_-_-_')\n",
    "#         print(images.shape)\n",
    "#         print(pred.shape)\n",
    "#         print(predictions[c:c + len(pred), ...].shape)\n",
    "#         print(c, len(pred))\n",
    "#         print('_-_-_-_-_-_-_-_-_')\n",
    "        predictions[c:c + len(pred), ...] = pred\n",
    "        c += pred.shape[0]\n",
    "\n",
    "    return predictions\n",
    "\n",
    "device = 'cuda'\n",
    "model = results.recorded_model.to(device)\n",
    "model = model.train()\n",
    "label_probabilities = get_label_probability(patches, model, standardize=True,\n",
    "                                            to_grayscale=False, device='cuda')\n",
    "probability_map = np.zeros(vs.frames_oa790[0].shape[:2], dtype=np.float32)\n",
    "rows, cols = np.unravel_index(vessel_pixel_indices, probability_map.shape[:2])\n",
    "probability_map[rows, cols] = label_probabilities[:, 1]\n",
    "plt.imshow(probability_map, cmap='hot')\n",
    "plt.scatter(vs.cell_positions[0][..., 0], vs.cell_positions[0][..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(probability_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "64 * 7 * 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset = ImageDataset(patches)\n",
    "loader = torch.utils.data.DataLoader(\n",
    "    image_dataset,\n",
    "    batch_size=1,\n",
    ")\n",
    "model.eval()\n",
    "for images in loader:\n",
    "    print(images.shape)\n",
    "    print(model.convolutional(images.to(device)).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking Negative Points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from nearest_neighbors import get_nearest_neighbor, get_nearest_neighbor_distances\n",
    "# https://matplotlib.org/api/_as_gen/matplotlib.patches.Circle.html\n",
    "\n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)\n",
    "vs = video_sessions[5]\n",
    "\n",
    "_, ax = plt.subplots(figsize=(11, 11))\n",
    "cx, cy = vs.cell_positions[0][0]\n",
    "r = 19\n",
    "ax.set_aspect(1)\n",
    "ax.scatter(vs.cell_positions[0][..., 0], vs.cell_positions[0][..., 1])\n",
    "ax.add_artist(matplotlib.patches.Circle((cx, cy), r, fill=False, edgecolor='r', linestyle=':'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points_on_circles(points, n_points_per_circle=1, ret_radii=False):\n",
    "    from nearest_neighbors import get_nearest_neighbor, get_nearest_neighbor_distances\n",
    "   \n",
    "    assert 1 <= n_points_per_circle <= 7, f'Points per circle must be between 1 and 7 not {n_points_per_circle}'\n",
    "  \n",
    "    neighbor_distances, nearest_neighbor_idxs = get_nearest_neighbor(points, 2)\n",
    "    \n",
    "    dist_flat = neighbor_distances.flatten()\n",
    "    dist_flat = np.delete(dist_flat, np.where(dist_flat > (dist_flat.mean() + 0 * dist_flat.std()))[0])\n",
    "    mean_distance = dist_flat.mean()\n",
    "\n",
    "    nnp = n_points_per_circle\n",
    "    uniform_angle_displacements = np.array([0, np.math.pi, np.math.pi * 0.5, np.math.pi * 3 * 0.5,  np.math.pi * 0.25, \n",
    "                                            3 * np.math.pi * 0.25, 5 * np.math.pi * 0.25, 7 * np.math.pi * 0.25]).squeeze()\n",
    "    c = 0\n",
    "    rxs = np.empty(len(points) * nnp)\n",
    "    rys = np.empty(len(points) * nnp)\n",
    "    radii = np.empty(len(points))\n",
    "    for centre_point_idx, (distances, neighbor_idxs) in enumerate(zip(neighbor_distances, nearest_neighbor_idxs)):\n",
    "        centre_point = points[centre_point_idx]\n",
    "        closest_point_1, closest_point_2 = points[neighbor_idxs]\n",
    "\n",
    "        r = (np.min(distances) *1.3) / 2\n",
    "        radii[centre_point_idx] = r\n",
    "        cx, cy = centre_point\n",
    "\n",
    "        angle = np.random.rand() * np.math.pi * 2;\n",
    "        random_angles = np.array(angle + uniform_angle_displacements).squeeze()\n",
    "\n",
    "        rx = np.array([cx + np.cos(random_angles[:nnp]) * r]).squeeze()\n",
    "        ry = np.array([cy + np.sin(random_angles[:nnp]) * r]).squeeze()\n",
    "\n",
    "        rxs[c:c + nnp] = rx\n",
    "        rys[c:c + nnp] = ry\n",
    "\n",
    "        c += nnp\n",
    "    \n",
    "    if ret_radii:\n",
    "        return rxs, rys, radii\n",
    "    else:\n",
    "        return rxs, rys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(11, 11))\n",
    "ax.scatter(points[..., 0], points[..., 1], c='b')\n",
    "ax.scatter(rxs, rys, c='r')\n",
    "\n",
    "for idx, r in enumerate(radii):\n",
    "    cx, cy = points[idx]\n",
    "    ax.add_artist(matplotlib.patches.Circle((cx, cy), r, fill=False, edgecolor='r', linestyle='--'))   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = vs.cell_positions[0]\n",
    "neighbor_distances, nearest_neighbor_idxs = get_nearest_neighbor(points, 2)\n",
    "\n",
    "_, ax = plt.subplots(figsize=(11, 11))\n",
    "cx, cy = vs.cell_positions[0][0]\n",
    "r = 19\n",
    "ax.set_aspect(1)\n",
    "ax.scatter(points[..., 0], points[..., 1])\n",
    "\n",
    "centre_point_idx = 6\n",
    "\n",
    "closest_point_1, closest_point_2 = points[[closest_point_1_idx, closest_point_2_idx]]\n",
    "ax.scatter(centre_point[0], centre_point[1], label='centre point')\n",
    "ax.scatter(closest_point_1[0], closest_point_1[1], label='other point')\n",
    "ax.scatter(closest_point_2[0], closest_point_2[1], label='other point')\n",
    "ax.legend()\n",
    "\n",
    "dist_flat = distances.flatten()\n",
    "dist_flat = np.delete(dist_flat, np.where(dist_flat > (dist_flat.mean() + 0 * dist_flat.std()))[0])\n",
    "mean_distance = dist_flat.mean()\n",
    "\n",
    "nnp = 3\n",
    "uniform_angle_displacements = np.array([0, np.math.pi, np.math.pi * 0.5, np.math.pi * 3 * 0.5,  np.math.pi * 0.25, \n",
    "                                         3 * np.math.pi * 0.25, 5 * np.math.pi * 0.25, 7 * np.math.pi * 0.25]).squeeze()\n",
    "\n",
    "rxs = np.empty(len(points) * nnp)\n",
    "rys = np.empty(len(points) * nnp)\n",
    "\n",
    "c = 0\n",
    "for centre_point_idx, (distances, neighbor_idxs) in enumerate(zip(neighbor_distances, nearest_neighbor_idxs)):\n",
    "    centre_point = points[centre_point_idx]\n",
    "    closest_point_1, closest_point_2 = points[neighbor_idxs]\n",
    "    \n",
    "    r = (np.min(distances) *1.3) / 2\n",
    "    cx, cy = centre_point\n",
    "    \n",
    "    angle = np.random.rand() * np.math.pi * 2;\n",
    "    random_point_angles = np.array(angle + uniform_angle_displacements).squeeze()\n",
    "    \n",
    "    rx = np.array([cx + np.cos(random_point_angles[:nnp]) * r]).squeeze()\n",
    "    ry = np.array([cy + np.sin(random_point_angles[:nnp]) * r]).squeeze()\n",
    "\n",
    "    rxs[c:c + nnp] = rx\n",
    "    rys[c:c + nnp] = ry\n",
    "    \n",
    "    c += nnp\n",
    "    \n",
    "    ax.scatter(rx, ry, c='r')\n",
    "    ax.scatter(rx_1, ry_1, c='r')\n",
    "\n",
    "    ax.add_artist(matplotlib.patches.Circle((centre_point[0], centre_point[1]), r, fill=False, edgecolor='b', linestyle=':'))   \n",
    "#     ax.add_artist(matplotlib.patches.Circle((centre_point[0], centre_point[1]), distances[0], fill=False, edgecolor='r', linestyle=':'))\n",
    "#     ax.add_artist(matplotlib.patches.Circle((centre_point[0], centre_point[1]), distances[1], fill=False, edgecolor='r', linestyle=':'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(11, 11))\n",
    "ax.scatter(rxs, rys, c='r')\n",
    "ax.scatter(points[..., 0], points[..., 1], c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_point_angles = [angle + np.array([0, np.math.pi, np.math.pi * 0.5, np.math.pi * 3 * 0.5 ])]\n",
    "cx + np.cos(random_point_angles[:4]) +  np.math.pi * r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(random_point_angles[:2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(distances_no_outliers.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances.flatten().shape, distances_no_outliers.flatten().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from cnnlearning import TrainingTracker\n",
    "\n",
    "results = TrainingTracker.from_file(os.path.join('tmp', 'results_file.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "with open(os.path.join('cache', 'models', 'blood_cell_classifier_va_0.847_ps_27_tw_0_mc_true_hm_false_npp_1_st_true_da_false', 'results.pkl'), 'rb') as results_file:\n",
    "    results = pickle.load(results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import get_cell_and_no_cell_patches\n",
    "from sharedvariables import get_video_sessions\n",
    "video_sessions = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)\n",
    "\n",
    "trainset, validset, cell_images, non_cell_images, _, _, _ =\\\n",
    "get_cell_and_no_cell_patches(\n",
    "        video_sessions=video_sessions,\n",
    "        patch_size=27,\n",
    "        temporal_width=0,\n",
    "        mixed_channel_patches=True,\n",
    "    \n",
    "        do_hist_match=False,\n",
    "        n_negatives_per_positive=1,\n",
    "\n",
    "        standardize_dataset=True,\n",
    "        apply_data_augmentation_to_dataset=False,\n",
    "\n",
    "        try_load_from_cache=True,\n",
    "\n",
    "        v=True, vv=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from classificationutils import classify_labeled_dataset, classify_images\n",
    "from learningutils import LabeledImageDataset, ImageDataset\n",
    "\n",
    "model = results.recorded_model\n",
    "model.eval()\n",
    "\n",
    "positive_dataset = LabeledImageDataset(cell_images[..., 1:],     np.ones(len(cell_images), dtype=np.int))\n",
    "negative_dataset = LabeledImageDataset(non_cell_images[..., 1:], np.zeros(len(non_cell_images), dtype=np.int))\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, model)\n",
    "\n",
    "_, positive_accuracy = classify_labeled_dataset(positive_dataset, model)\n",
    "_, negative_accuracy = classify_labeled_dataset(negative_dataset, model)\n",
    "\n",
    "print()\n",
    "print(f'Model trained on {len(cell_images)} cell patches and {len(non_cell_images)} non cell patches.')\n",
    "print()\n",
    "print('Brief evaluation - best validation accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')\n",
    "\n",
    "train_model = results.recorded_train_model\n",
    "train_model.eval()\n",
    "\n",
    "_, train_accuracy = classify_labeled_dataset(trainset, train_model)\n",
    "_, valid_accuracy = classify_labeled_dataset(validset, train_model)\n",
    "\n",
    "\n",
    "_, positive_accuracy = classify_labeled_dataset(positive_dataset, train_model)\n",
    "_, negative_accuracy = classify_labeled_dataset(negative_dataset, train_model)\n",
    "\n",
    "print()\n",
    "print('Brief evaluation - best training accuracy model')\n",
    "print('----------------')\n",
    "print(f'Epoch:\\t', results.recorded_train_model_epoch)\n",
    "print('Training accuracy:\\t', f'{train_accuracy:.3f}')\n",
    "print('Validation accuracy:\\t', f'{valid_accuracy:.3f}')\n",
    "print()\n",
    "print('Positive accuracy:\\t', f'{positive_accuracy:.3f}')\n",
    "print('Negative accuracy:\\t', f'{negative_accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flow quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classificationutils import create_probability_map\n",
    "from patchextraction import extract_patches\n",
    "from imageprosessing import std_image\n",
    "\n",
    "frame = vs.frames_oa790[0]\n",
    "extract_patches(frame).shape\n",
    "\n",
    "std_image_oa850    = std_image(vs.frames_oa850, vs.mask_frames_oa850, sigma=0, method='j_tam', adapt_hist=True)\n",
    "std_image_confocal = std_image(vs.frames_confocal, vs.mask_frames_confocal, sigma=0, method='j_tam', adapt_hist=True)\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(std_image_oa850)\n",
    "plt.subplot(122)\n",
    "plt.imshow(std_image_confocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imageprosessing import ImageRegistator\n",
    "from vesseldetection import create_vessel_mask\n",
    "\n",
    "confocal_mask = create_vessel_mask(std_image_confocal)\n",
    "oa850_mask = create_vessel_mask(std_image_oa850)\n",
    "plt.subplot(121)\n",
    "plt.imshow(confocal_mask, label='Confocal mask')\n",
    "plt.subplot(122)\n",
    "plt.imshow(oa850_mask, label='oa850 mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "vs = video_sessions[0]\n",
    "plt.imshow(vs.frames_oa790[0], cmap='gray')\n",
    "plt.imshow(vs.registered_frames_oa850[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(vs.vessel_mask_confocal)\n",
    "vs.vessel_mask_confocal_file = ''\n",
    "vs.vessel_mask_confocal = None\n",
    "plt.subplot(122)\n",
    "plt.imshow(vs.vessel_mask_confocal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vesseldetection import create_vessel_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ir = ImageRegistator(source=oa850_mask, target=confocal_mask)\n",
    "ir.register_vertically()\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(confocal_mask, label='Confocal mask')\n",
    "plt.subplot(122)\n",
    "plt.imshow(ir.registered_source, label='oa850 mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = frame\n",
    "patch_size = (21, 21)\n",
    "patch_height, patch_width = patch_size\n",
    "\n",
    "kernel_height, kernel_width = patch_height, patch_width\n",
    "\n",
    "inp = torch.from_numpy(img)\n",
    "\n",
    "if len(inp.shape) == 3:\n",
    "    inp = inp.permute(-1, 0, 1)\n",
    "elif len(inp.shape) == 2:\n",
    "    inp = inp[None, ...]\n",
    "inp = inp[None, ...]\n",
    "\n",
    "print(\"Inp.shape\", inp.shape)\n",
    "patches = inp.unfold(0, 0, 1).unfold(2, kernel_height, 1).unfold(3, kernel_width, 1)\n",
    "# Shape -> 1 x 1 x H x W x Hpatch x Wpatch\n",
    "print(\"Patches shape 1\", patches.shape)\n",
    "\n",
    "patches = patches.permute(2, 3, 1, -2, -1, 0)[..., 0]\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying preprocessing to session frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.Image\n",
    "video_sessions = get_video_sessions(marked=True, validation=True)\n",
    "\n",
    "PIL.Image.fromarray(vs.frames_oa790[0]).save('frame_790_registered.png')\n",
    "PIL.Image.fromarray(vs.frames_oa790[0] * vs.mask_frames_oa790[0]).save('frame_790_registered_masked.png')\n",
    "PIL.Image.fromarray(vs.mask_frames_oa790[0].astype(np.uint8) * 255).save('frame_790_registered_mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from plotutils import *\n",
    "from imageprosessing import SessionPreprocessor, enhance_motion_contrast, normalize_data\n",
    "\n",
    "\n",
    "video_sessions = get_video_sessions(marked=True, registered=True)\n",
    "vs = video_sessions[3]\n",
    "\n",
    "preprocessor = SessionPreprocessor(vs, [\n",
    "    lambda frames: enhance_motion_contrast(frames, sigma=1.2),\n",
    "    lambda frames: normalize_data(frames, (0, 255)).astype(np.uint8)\n",
    "])\n",
    "\n",
    "plt\n",
    "plt.subplot(121)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.scatter(vs.cell_positions[0][:, 0], vs.cell_positions[0][:, 1], s=5)\n",
    "plt.title('Before applying preprocessing', fontsize=15)\n",
    "\n",
    "preprocessor.apply_preprocessing()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.scatter(vs.cell_positions[0][:, 0], vs.cell_positions[0][:, 1],  s=5)\n",
    "\n",
    "plt.title('After applying preprocessing', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.apply_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(75, 75))\n",
    "plt.subplot(121)\n",
    "plt.imshow(vs.frames_oa790[0], cmap='gray')\n",
    "plt.scatter(vs.cell_positions[0][:, 0], vs.cell_positions[0][:, 1], s=5)\n",
    "plt.title('Before applying preprocessing', fontsize=15)\n",
    "\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(vs.frames_oa790[0], cmap='gray')\n",
    "plt.scatter(vs.cell_positions[0][:, 0], vs.cell_positions[0][:, 1],  s=5)\n",
    "\n",
    "plt.title('After applying preprocessing', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply motion contrast enhancement to all video sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from imageprosessing import SessionPreprocessor, enhance_motion_contrast\n",
    "import tqdm\n",
    "video_sessions_enhanced = get_video_sessions(should_have_marked_cells=True, should_be_registered=True)\n",
    "\n",
    "for vs in tqdm.tqdm(video_sessions_enhanced):\n",
    "    preprocessor = SessionPreprocessor(vs, lambda frames: enhance_motion_contrast(frames, sigma=1))\n",
    "    preprocessor.apply_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_datasets import create_cell_and_no_cell_patches\n",
    "cell_images_enhanced, non_cell_images_enhanced, cell_images_marked, non_cell_images_marked =\\\n",
    "create_cell_and_no_cell_patches(\n",
    "    video_sessions=video_sessions_enhanced\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotutils import plot_images_as_grid\n",
    "plot_images_as_grid(cell_images_enhanced[:10])\n",
    "plot_images_as_grid(cell_images_marked[:10])\n",
    "plot_images_as_grid(non_cell_images_enhanced[:10])\n",
    "plot_images_as_grid(non_cell_images_marked[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "from cnnlearning import CNN, train, TrainingTracker \n",
    "from generate_datasets import create_dataset_from_cell_and_no_cell_images\n",
    "\n",
    "trainset, validset = create_dataset_from_cell_and_no_cell_images(cell_images_enhanced, \n",
    "                                                                 non_cell_images_enhanced,\n",
    "                                                                 standardize=True)\n",
    "model = CNN(dataset_sample=trainset, output_classes=2).to('cuda')\n",
    "train_params = collections.OrderedDict(\n",
    "    epochs=250,\n",
    "    lr = .001,\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    batch_size='all', # can be a number or None/'all' to train all trainset at once. \n",
    "    do_early_stop=True,  # Optional default True\n",
    "    early_stop_patience=60, # How many epochs with no validation accuracy improvement before stopping early\n",
    "    learning_rate_scheduler_patience=20, # How many epochs with no validation accuracy improvement before lowering learning rate\n",
    "    evaluate_epochs=10,\n",
    "    \n",
    "    trainset=trainset,\n",
    "    validset=validset,\n",
    "    shuffle=True)\n",
    "results: TrainingTracker = train(model,\n",
    "                                 train_params,\n",
    "                                 criterion=torch.nn.CrossEntropyLoss(),\n",
    "                                 device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from imageprosessing import SessionPreprocessor, enhance_motion_contrast, normalize_data, frame_differencing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "\n",
    "preprocessor = SessionPreprocessor(vs, [\n",
    "    lambda frames: frame_differencing(frames, sigma=1.2),\n",
    "    lambda frames: np.uint8(normalize_data(frames, (0, 255))),\n",
    "    lambda frames: enhance_motion_contrast(frames, sigma=1.2)]\n",
    "                                  )\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.title('Before applying preprocessing', fontsize=15)\n",
    "\n",
    "preprocessor.apply_preprocessing_to_oa790()\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.title('After applying preprocessing', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "plt.imshow(filters.gaussian(vs.masked_frames_oa790[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blur_images(vs.masked_frames_oa790).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from imageprosessing import SessionPreprocessor, equalize_adapthist_images\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from skimage import filters\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "def blur_images(frames, sigma=1):\n",
    "    blurred_images = np.empty_like(frames, dtype=np.float64)\n",
    "    for i, im in enumerate(frames):\n",
    "        blurred_images[i] = filters.gaussian(im, sigma)\n",
    "    return blurred_images\n",
    "\n",
    "preprocessor = SessionPreprocessor(vs, [\n",
    "    lambda frames: blur_images(frames, sigma=2),\n",
    "    lambda frames: np.uint8(frames * 255),\n",
    "    equalize_adapthist_images,\n",
    "    lambda frames: np.ma.array([filters.unsharp_mask(f, radius=5, amount=1, preserve_range=True) for f in frames], dtype=np.uint8),\n",
    "])\n",
    "\n",
    "_, axes = plt.subplots(1, 2, figsize=(60, 70))\n",
    "axes[0].imshow(vs.frames_oa790[0], cmap='gray')\n",
    "axes[0].set_title('Before applying preprocessing', fontsize=15)\n",
    "\n",
    "preprocessor.apply_preprocessing_to_oa790()\n",
    "\n",
    "axes[1].imshow(vs.frames_oa790[0], cmap='gray')\n",
    "axes[1].set_title('After applying preprocessing', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in [\n",
    "    lambda frames: frame_differencing(frames, sigma=1.2),\n",
    "    lambda frames: enhance_motion_contrast(frames, sigma=1.2),\n",
    "                                       ]:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from imageprosessing import SessionPreprocessor, enhance_motion_contrast\n",
    "import mahotas as mh\n",
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "plt.imshow(mh.gaussian_filter(vs.masked_frames_oa790.mean(0), sigma=2))\n",
    "# preprocessor = SessionPreprocessor(vs, lambda frames, masks: np.ma.mean(frames, masks))\n",
    "\n",
    "# plt.subplot(121)\n",
    "# plt.imshow(vs.frames_oa790[0])\n",
    "# plt.title('Before applying preprocessing', fontsize=15)\n",
    "\n",
    "# preprocessor.apply_preprocessing()\n",
    "\n",
    "# plt.subplot(122)\n",
    "# plt.imshow(vs.frames_oa790[0])\n",
    "# plt.title('After applyging preprocessing', fontsize=15)\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'hey'\n",
    "import collections\n",
    "print(isinstance(string, List))\n",
    "for s in string:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'Hey'\n",
    "list(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(frame_differencing(vs.masked_frames_oa790, 2)[3])\n",
    "frame_differencing(vs.masked_frames_oa790, 2).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_sessions = get_video_sessions()\n",
    "\n",
    "vs = [vs for vs in video_sessions if 'shared-videos' in vs.video_file][0]\n",
    "print(vs)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.show()\n",
    "extractor = SessionPatchExtractor(vs, patch_size=21)\n",
    "preprocessor = SessionPreprocessor(vs, lambda frames, masks: enhance_motion_contrast(frames, masks, sigma=0.125, mask_crop_pixels=0))\n",
    "preprocessor.apply_preprocessing_to_oa790()\n",
    "plt.imshow(vs.frames_oa790[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(vs.frames_oa790[23])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from imageprosessing import SessionPreprocessor, enhance_motion_contrast\n",
    "from patchextraction import SessionPatchExtractor\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "extractor = SessionPatchExtractor(vs, patch_size=21)\n",
    "preprocessor = SessionPreprocessor(vs, lambda frames, masks: enhance_motion_contrast(frames, masks, sigma=1.2, mask_crop_pixels=0))\n",
    "\n",
    "cell_patches_before_preprocessing = extractor.cell_patches_oa790\n",
    "non_cell_patches_before_preprocessing = extractor.non_cell_patches_oa790\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.title('Before applying preprocessing', fontsize=15)\n",
    "plt.scatter(vs.cell_positions[0][:, 0], vs.cell_positions[0][:, 1])\n",
    "\n",
    "preprocessor.apply_preprocessing()\n",
    "\n",
    "extractor._reset_patches()\n",
    "cell_patches_after_preprocessing = extractor.cell_patches_oa790\n",
    "non_cell_patches_after_preprocessing = extractor.non_cell_patches_oa790\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.imshow(vs.frames_oa790[0])\n",
    "plt.title('After applying preprocessing', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images_as_grid(cell_patches_before_preprocessing[:10])\n",
    "plot_images_as_grid(non_cell_patches_before_preprocessing[:10])\n",
    "\n",
    "plot_images_as_grid(cell_patches_after_preprocessing[:10])\n",
    "plot_images_as_grid(non_cell_patches_after_preprocessing[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Channel oa850')\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_cell_patches[:10][..., 1])\n",
    "# plot_images_as_grid(patch_extractor.mixed_channel_marked_cell_patches[:10][..., 1])\n",
    "\n",
    "plot_images_as_grid(patch_extractor.mixed_channel_non_cell_patches[:10][..., 1])\n",
    "# plot_images_as_grid(patch_extractor.mixed_channel_marked_non_cell_patches[:10][..., 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " patch_extractor.temporal_marked_cell_patches_oa790[1].transpose(2, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import SessionPatchExtractor\n",
    "from plotutils import plot_images_as_grid, no_ticks\n",
    "import numpy as np\n",
    "\n",
    "video_sessions = get_video_sessions(should_have_marked_video=True)\n",
    "vs = video_sessions[0]\n",
    "\n",
    "patch_extractor = SessionPatchExtractor(vs, patch_size=37)\n",
    "patch_extractor.patch_size =31\n",
    "print(patch_extractor.temporal_cell_patches_oa790.max())\n",
    "print(patch_extractor.temporal_marked_cell_patches_oa790.max())\n",
    "\n",
    "_, axes = plt.subplots(1, 3)\n",
    "no_ticks(axes)\n",
    "for ax, channel_patch in zip(axes, patch_extractor.temporal_marked_cell_patches_oa790[29].transpose(2, 0, 1)):\n",
    "    ax.imshow(channel_patch, cmap='gray')\n",
    "    \n",
    "# _, axes = plt.subplots(1, 3)\n",
    "# no_ticks(axes)\n",
    "# for ax, channel_patch in zip(axes, patch_extractor.temporal_non_cell_patches_oa790[0].transpose(2, 0, 1)):\n",
    "#     ax.imshow(channel_patch, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " patch_extractor.temporal_non_cell_patches_oa790"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_extractor.temporal_marked_cell_patches_oa790[1].transpose(2, 0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get all the patches extracted\n",
    "plot_images_as_grid(patch_extractor.cell_patches_oa790)\n",
    "plot_images_as_grid(patch_extractor.marked_cell_patches_oa790)\n",
    "\n",
    "plot_images_as_grid(patch_extractor.non_cell_patches_oa790)\n",
    "plot_images_as_grid(patch_extractor.marked_non_cell_patches_oa790)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to get standard deviation image and vessel masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(40, 10))\n",
    "\n",
    "# If empty string then file not found\n",
    "if session.std_image_oa850_file != \"\":\n",
    "    axes[0].imshow(session.std_image_oa850)\n",
    "    axes[0].set_title('Std image oa850', fontsize=20)\n",
    "\n",
    "if session.std_image_confocal_file != \"\":\n",
    "    axes[1].imshow(session.std_image_confocal)\n",
    "    axes[1].set_title('Std image confocal', fontsize=20)\n",
    "\n",
    "if session.vessel_mask_oa850_file != '':\n",
    "    axes[2].imshow(session.vessel_mask_oa850)\n",
    "    axes[2].set_title('Vessel mask oa850', fontsize=20)\n",
    "\n",
    "if session.vessel_mask_confocal_file != '':\n",
    "    axes[3].imshow(session.vessel_mask_confocal)\n",
    "    axes[3].set_title('Vessel mask confocal', fontsize=20)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voronoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sharedvariables import get_video_sessions\n",
    "from patchextraction import extract_patches\n",
    "from scipy.spatial import Voronoi, voronoi_plot_2d\n",
    "import numpy as np\n",
    "vs = get_video_sessions(should_be_registered=True, should_have_marked_cells=True)[2]\n",
    "points = vs.cell_positions[0]\n",
    "vor = Voronoi(points)\n",
    "fig = voronoi_plot_2d(vor)\n",
    "def get_random_points_in_voronoi_diagram(centroids, limits):\n",
    "    \n",
    "    vor = Voronoi(centroids, qhull_options='Qbb Qc Qx', incremental=False)\n",
    "    vor.close()\n",
    "\n",
    "    edges = np.array(vor.ridge_vertices)\n",
    "\n",
    "    edges_start = edges[:, 0]\n",
    "    edges_end = edges[:, 1]\n",
    "\n",
    "    vertices_start = vor.vertices[edges_start]\n",
    "    print(vertices_start.shape)\n",
    "    vertices_end = vor.vertices[edges_end]\n",
    "\n",
    "    t = np.random.rand(vertices_start.shape[0])\n",
    "\n",
    "    random_vertices = t[:, np.newaxis] * vertices_start + (1 - t[:, np.newaxis]) * vertices_end\n",
    "    random_vertices = random_vertices[edges_start != -1]\n",
    "\n",
    "    random_vertices = random_vertices[random_vertices[:, 0] >= 0]\n",
    "    random_vertices = random_vertices[random_vertices[:, 0] <= limits[1]]\n",
    "    random_vertices = random_vertices[random_vertices[:, 1] >= 0]\n",
    "    random_vertices = random_vertices[random_vertices[:, 1] <= limits[0]]\n",
    "\n",
    "    return random_vertices\n",
    "# random_points = get_random_points_in_voronoi_diagram(points, vs.frames_oa790[0].shape[:2])\n",
    "plt.imshow(np.uint8(~vs.vessel_mask_confocal)*255, cmap='gray')\n",
    "# plt.scatter(random_points[:, 0], random_points[:, 1], c='r', s=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.uint8(~vs.vessel_mask_confocal)*0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vs.mask_frames_oa790[0].shape, vs.frames_oa790.shape, vs.masked_frames_oa790.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from videoutils import get_frames_from_video\n",
    "\n",
    "get_frames_from_video(vs.mask_video_oa790_file).shape, get_frames_from_video(vs.video_oa790_file).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.ma.masked_array(self.frames_oa790, ~self.mask_frames_oa790)\n",
    "vs.mask_frames_oa790.shape, vs.frames_oa790.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
